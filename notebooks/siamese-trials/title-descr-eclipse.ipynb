{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Identify Duplicate Bug Reports Using Siamese Cross-Encoder Network"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import itertools"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def load_dataset(limit=0, verbose=False):\n",
    "  client = MongoClient()\n",
    "  db = client['netBeans']\n",
    "  bug_collection = db['clear']\n",
    "  pairs_collection = db['pairs']\n",
    "\n",
    "  pairs: Tuple[Dict] = tuple(pairs_collection.find(limit=limit))\n",
    "  if verbose:\n",
    "    print('total pairs', len(pairs))\n",
    "  bug_groups = [[pair['bug1'], pair['bug2']] for pair in pairs]\n",
    "  candidate_bug_ids = [\n",
    "    str(bug_id)\n",
    "    for bug_group in bug_groups for bug_id in bug_group\n",
    "  ]\n",
    "  if verbose:\n",
    "    print('total candidate_bug_ids', len(candidate_bug_ids))\n",
    "\n",
    "  # Storing bug reports as dictionary so that they can be\n",
    "  # retrieved by bug_id\n",
    "  bug_reports: Dict[str, Dict] = {}\n",
    "  for bug_report in bug_collection.find():\n",
    "    bug_reports[bug_report['bug_id']] = bug_report\n",
    "  if verbose:\n",
    "    print('total bug_reports', len(bug_reports))\n",
    "\n",
    "  return bug_reports, pairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def create_dataframe(bug_reports: Dict[str, Dict], pairs: Tuple[Dict]):\n",
    "  data = []\n",
    "  for pair in pairs:\n",
    "    bug1_id = str(pair['bug1'])\n",
    "    bug2_id = str(pair['bug2'])\n",
    "    if bug1_id in bug_reports and bug2_id in bug_reports:\n",
    "      bug1 = bug_reports[bug1_id]\n",
    "      bug2 = bug_reports[bug2_id]\n",
    "      data.append([\n",
    "        bug1['short_desc'],\n",
    "        bug2['short_desc'],\n",
    "        bug1['description'] if 'description' in bug1 else '',\n",
    "        bug2['description'] if 'description' in bug2 else '',\n",
    "        False if pair['dec'] == -1 else True])\n",
    "\n",
    "  columns = ['title1', 'title2', 'description1', 'description2', 'is_similar']\n",
    "\n",
    "  return pd.DataFrame(data=data, columns=columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def unidecode_str_cols(df: pd.DataFrame):\n",
    "  from unidecode import unidecode\n",
    "\n",
    "  def decode_col(col: pd.Series):\n",
    "    if pd.api.types.is_string_dtype(col.dtype):\n",
    "      # unidecode throws following weird error on empty string\n",
    "      # 'list' object has no attribute 'encode'\n",
    "      return col.apply(lambda sent: unidecode(sent) if sent else sent)\n",
    "    return col\n",
    "\n",
    "  return df.apply(decode_col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dup_df = unidecode_str_cols(create_dataframe(*load_dataset()))\n",
    "print('Data shape:', dup_df.shape)\n",
    "dup_df.sample(n=10, random_state=13)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Data Types:')\n",
    "dup_df.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "FEATURES = ['title1', 'title2', 'description1', 'description2']\n",
    "print('Description of length of the feature columns')\n",
    "dup_df[FEATURES].apply(lambda col: col.str.len().describe())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Outliers by length:')\n",
    "\n",
    "\n",
    "def count_tail_outliers(col: pd.Series):\n",
    "  lengths: pd.Series = col.str.len()\n",
    "  iqr = lengths.quantile(0.75) - lengths.quantile(0.25)\n",
    "  outlier_range = lengths.quantile(0.75) + 1.5 * iqr\n",
    "  outlier_count = sum(lengths > outlier_range)\n",
    "  return pd.Series({\n",
    "    'iqr': iqr,\n",
    "    'count': outlier_count,\n",
    "    'frac': outlier_range / len(lengths),\n",
    "  })\n",
    "\n",
    "\n",
    "dup_df[FEATURES].apply(count_tail_outliers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filter out entries of insuficient length"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dup_df = dup_df[\n",
    "  (dup_df.title1.str.len() >= 10)\n",
    "  & (dup_df.title2.str.len() >= 10)\n",
    "  & (dup_df.description1.str.len() >= 50)\n",
    "  & (dup_df.description2.str.len() >= 50)\n",
    "  ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Shape after drop:', dup_df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dup_df.groupby(by='is_similar').apply(\n",
    "  lambda group: pd.Series({\n",
    "    'count': group.size,\n",
    "    'frac': len(group) / len(dup_df),\n",
    "  }),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train, Validation, Test Split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_val_df, test_df = train_test_split(\n",
    "  dup_df,\n",
    "  test_size=20000,\n",
    "  stratify=dup_df.is_similar,\n",
    "  random_state=13,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "  train_val_df,\n",
    "  test_size=20000,\n",
    "  stratify=train_val_df.is_similar,\n",
    "  random_state=13,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Train Val Test Size: {len(train_df):,} {len(val_df):,} {len(test_df):,}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download & Prepare Embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.utils as kutils\n",
    "from keras.layers.preprocessing.text_vectorization import TextVectorization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ensure_glove_embedding(verbose=False):\n",
    "  import pathlib\n",
    "  embedding_data_path = kutils.get_file(\n",
    "    'glove.42B.300d.zip',\n",
    "    'https://nlp.stanford.edu/data/glove.42B.300d.zip',\n",
    "    untar=True,\n",
    "    extract=True,\n",
    "  )\n",
    "\n",
    "  # If this operation fails, print the parent-dir\n",
    "  # go there, and extract the file\n",
    "  file_path = pathlib.Path(embedding_data_path).parent / 'glove.42B.300d.txt'\n",
    "\n",
    "  if verbose:\n",
    "    with open(file_path, encoding='utf-8') as glove_embedding_file:\n",
    "      for i in range(5):\n",
    "        line = glove_embedding_file.readline()\n",
    "        word, *embedding = line.split()\n",
    "        print(\n",
    "          'Word:', word,\n",
    "          '| Embedding length:', len(embedding),\n",
    "          '| Average embedding:', sum(map(float, embedding)) / len(embedding),\n",
    "        )\n",
    "\n",
    "  return file_path\n",
    "\n",
    "\n",
    "glove_file_path = ensure_glove_embedding(verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Embedding Index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_embedding_index(embedding_file_path: str, verbose=False):\n",
    "  if verbose:\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "    # there are 1.9M words, and we will update progress\n",
    "    # on every 1000 word read\n",
    "    progress_bar = tqdm(total=1917494)\n",
    "\n",
    "  embedding_index: Dict[str, np.ndarray] = {}\n",
    "  with open(embedding_file_path, encoding='utf-8') as embedding_file:\n",
    "    i = 0\n",
    "    for line in embedding_file:\n",
    "      i += 1\n",
    "      word, coefficients = line.split(maxsplit=1)\n",
    "      if i > 1917494:\n",
    "        print('word:', word)\n",
    "        break\n",
    "      coefficients = np.fromstring(coefficients, 'float', sep=' ')\n",
    "      embedding_index[word] = coefficients\n",
    "\n",
    "      if verbose:\n",
    "        if i % 1000 == 0:\n",
    "          progress_bar.update(1000)\n",
    "\n",
    "  if verbose:\n",
    "    progress_bar.close()\n",
    "\n",
    "  if verbose:\n",
    "    print(f'Found {len(embedding_index)} words in the embedding.')\n",
    "    print(f'Embedding dimension: {len(next(iter(embedding_index.values())))}')\n",
    "\n",
    "  return embedding_index\n",
    "\n",
    "\n",
    "embedding_index = create_embedding_index(glove_file_path, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Vocabulary Index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAX_TOKENS = 20000\n",
    "MAX_TITLE_LENGTH = 21\n",
    "MAX_DESCRIPTION_LENGTH = 200\n",
    "EMBEDDING_DIM = 300"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_vocab(sentences: List[str], sequence_length: int):\n",
    "  vectorizer = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS - 2,\n",
    "    output_sequence_length=sequence_length,\n",
    "  )\n",
    "  vectorizer.adapt(sentences)\n",
    "  vocab = vectorizer.get_vocabulary()\n",
    "  word_index = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "  return vectorizer, word_index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "title_vectorizer, title_word_index = build_vocab(\n",
    "  [*dup_df.title1, *dup_df.title2],\n",
    "  MAX_TITLE_LENGTH,\n",
    ")\n",
    "\n",
    "print(\n",
    "  'Most frequent title words:',\n",
    "  list(itertools.islice(title_word_index.keys(), 5)),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "descr_vectorizer, descr_word_index = build_vocab(\n",
    "  [*dup_df.description1, *dup_df.description2],\n",
    "  MAX_DESCRIPTION_LENGTH,\n",
    ")\n",
    "\n",
    "print(\n",
    "  'Most frequent description words:',\n",
    "  list(itertools.islice(descr_word_index.keys(), 5)),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Embedding Matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_embedding_matrix(\n",
    "  embedding_index: Dict[str, np.ndarray],\n",
    "  word_index: Dict[str, int],\n",
    "  verbose=False,\n",
    "):\n",
    "  hits = 0\n",
    "  misses = 0\n",
    "\n",
    "  # prepare embedding matrix\n",
    "  embedding_matrix = np.zeros((MAX_TOKENS, EMBEDDING_DIM))\n",
    "  for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      # Words not found in embedding index will be all-zeros.\n",
    "      # This includes the representation for \"padding\" and \"OOV\"\n",
    "      embedding_matrix[i] = embedding_vector\n",
    "      hits += 1\n",
    "    else:\n",
    "      misses += 1\n",
    "\n",
    "  if verbose:\n",
    "    print('Embedding shape:', embedding_matrix.shape)\n",
    "    print(f'Found {hits} words, missed {misses}.')\n",
    "\n",
    "  return embedding_matrix\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Creating title embedding matrix:')\n",
    "title_embedding_matrix = create_embedding_matrix(\n",
    "  embedding_index, title_word_index, True,\n",
    ")\n",
    "\n",
    "print('\\nCreating description embedding matrix:')\n",
    "descr_embedding_matrix = create_embedding_matrix(\n",
    "  embedding_index, descr_word_index, True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Training Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_batches(\n",
    "  split_df: pd.DataFrame,\n",
    "  batch_size=1024,\n",
    "):\n",
    "  steps_per_epoch = len(split_df) // batch_size\n",
    "  while True:\n",
    "    for i in range(steps_per_epoch):\n",
    "      offset = i * batch_size\n",
    "      till = offset + batch_size\n",
    "      feature_batches = []\n",
    "      for feature in FEATURES:\n",
    "        vectorizer = title_vectorizer if feature.startswith('title') else descr_vectorizer\n",
    "        feature_batch = vectorizer(\n",
    "          split_df[feature][offset: till].to_numpy().reshape((-1, 1))\n",
    "        ).numpy()\n",
    "        feature_batches.append(feature_batch)\n",
    "\n",
    "\n",
    "      target_batch = split_df.is_similar[offset: till].to_numpy()\n",
    "      yield (\n",
    "        feature_batches,\n",
    "        target_batch,\n",
    "      )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import callbacks\n",
    "from keras.initializers.initializers_v2 import Constant"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TitleEmbeddingLayer = layers.Embedding(\n",
    "  input_dim=MAX_TOKENS,\n",
    "  output_dim=EMBEDDING_DIM,\n",
    "  embeddings_initializer=Constant(title_embedding_matrix),\n",
    "  trainable=False,\n",
    "  name='TitleEmbeddingLayer',\n",
    ")\n",
    "\n",
    "TitleLSTMLayer = layers.Bidirectional(layers.LSTM(\n",
    "  units=50,\n",
    "  dropout=0.2,\n",
    "  recurrent_dropout=0.2,\n",
    "), name='TitleBidirectionalLSTMLayer')\n",
    "\n",
    "DescrEmbeddingLayer = layers.Embedding(\n",
    "  input_dim=MAX_TOKENS,\n",
    "  output_dim=EMBEDDING_DIM,\n",
    "  embeddings_initializer=Constant(descr_embedding_matrix),\n",
    "  trainable=False,\n",
    "  name='DescrEmbeddingLayer',\n",
    ")\n",
    "\n",
    "def make_descr_layer(num, kernel_size=3, pool_size=2, strides=None):\n",
    "  DescrConv1dLayer = layers.Conv1D(\n",
    "    filters=32,\n",
    "    kernel_size=kernel_size,\n",
    "    activation='relu',\n",
    "    name=f'DescrConv1dLayer{num}',\n",
    "  )\n",
    "  DescrMaxPool1dLayer = layers.MaxPool1D(\n",
    "    pool_size=pool_size,\n",
    "    strides=strides,\n",
    "    name=f'DescrMaxPool1dLayer{num}',\n",
    "  )\n",
    "  return DescrConv1dLayer, DescrMaxPool1dLayer\n",
    "\n",
    "DescrConv1dLayer1, DescrMaxPool1dLayer1 = make_descr_layer(1, pool_size=4)\n",
    "DescrConv1dLayer2, DescrMaxPool1dLayer2 = make_descr_layer(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_siamese_component(num: int):\n",
    "  class SiameseComponent:\n",
    "    def __init__(self, title: layers.Input, description: layers.Input, output: layers.Concatenate):\n",
    "      self.title = title\n",
    "      self.description = description\n",
    "      self.output = output\n",
    "\n",
    "  title_input = layers.Input(shape=(None,), dtype='int32', name=f'title{num}_input')\n",
    "  title_embedding_layer = TitleEmbeddingLayer(title_input)\n",
    "  title_lstm_layer = TitleLSTMLayer(title_embedding_layer)\n",
    "\n",
    "  descr_input = layers.Input(shape=(None,), name=f'descr{num}_input')\n",
    "  descr_embedding_layer = DescrEmbeddingLayer(descr_input)\n",
    "  descr_conv1d1 = DescrConv1dLayer1(descr_embedding_layer)\n",
    "  descr_max_pool1d1 = DescrMaxPool1dLayer1(descr_conv1d1)\n",
    "  descr_conv1d2 = DescrConv1dLayer2(descr_max_pool1d1)\n",
    "  descr_max_pool1d2 = DescrMaxPool1dLayer2(descr_conv1d2)\n",
    "  # descr_conv1d3 = DescrConv1dLayer3(descr_max_pool1d2)\n",
    "  # descr_max_pool1d3 = DescrMaxPool1dLayer3(descr_conv1d3)\n",
    "  descr_flat_Layer = layers.Flatten(name=f'FlatDescr{num}')(descr_max_pool1d2)\n",
    "\n",
    "  concat = layers.Concatenate(axis=1, name=f'Concat{num}')([title_lstm_layer, descr_flat_Layer])\n",
    "  return SiameseComponent(title_input, descr_input, concat)\n",
    "\n",
    "\n",
    "component1 = create_siamese_component(1)\n",
    "component2 = create_siamese_component(2)\n",
    "\n",
    "dot_product_layer = layers.Dot(\n",
    "  axes=1,\n",
    "  name='dot_product_layer'\n",
    ")([component1.output, component2.output])\n",
    "output = layers.Dense(\n",
    "  1, activation='sigmoid', name='output',\n",
    ")(dot_product_layer)\n",
    "siamese_model = models.Model(\n",
    "  inputs=[component1.title, component2.title, component1.description, component2.description],\n",
    "  outputs=output,\n",
    "  name='siamese_model'\n",
    ")\n",
    "\n",
    "kutils.plot_model(siamese_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "\n",
    "siamese_model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['acc'],\n",
    ")\n",
    "callback = callbacks.ModelCheckpoint(\n",
    "    filepath=f'../../models/siamese-netbeans'\n",
    "             '.epoch-{epoch:02d}-loss-{val_loss:.3f}.hdf5',\n",
    "    monitor='val_loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    ")\n",
    "history = siamese_model.fit(\n",
    "  generate_batches(train_df, BATCH_SIZE),\n",
    "  steps_per_epoch=len(train_df) // BATCH_SIZE,\n",
    "  epochs=30,\n",
    "  validation_data=generate_batches(val_df, BATCH_SIZE),\n",
    "  validation_steps=len(val_df) // BATCH_SIZE,\n",
    "  verbose=1,\n",
    "  callbacks=[callback],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ax = pd.DataFrame(history.history).plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}